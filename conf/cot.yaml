# Model & Tokenizer
model:
  # meta-llama/Llama-3.2-3B-Instruct
  # mistralai/Mistral-7B-Instruct-v0.3
  name: "google/gemma-2-2b-it"
  torch_dtype: "bfloat16"

  quant:
    load_in_4bit: true
    quant_type: "nf4"
    compute_dtype: "bfloat16"
    double_quant: true

tokenizer:
  padding_side: "right"
  pad_token_as_eos: true



# Dataset
data:
  name: "halueval"                         
  path: "../cot/data/halueval/cot_finetune_data_2steps.jsonl"
  text_field: "messages"
  split_ratio: 0.1
  max_seq_length: 1500
  remove_system_for_gemma: true



# Prompt Formatting

prompt:
  use_chat_template: true
  add_generation_prompt: true
  max_length: 14000



# LoRA Configuration
lora:
  enable: true
  r: 16
  alpha: 16
  dropout: 0.2
  bias: "none"
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj



# Training Settings
training:
  checkpoint_dir: "../cot/cot_model/halueval/gemma/checkpoints"

  per_device_train_batch_size: 12
  gradient_accumulation_steps: 4
  num_train_epochs: 10
  learning_rate: 2e-5
  lr_scheduler_type: "cosine"
  warmup_steps: 100
  warmup_ratio: 0.1

  fp16: true
  gradient_checkpointing: true
  optim: "paged_adamw_32bit"
  logging_steps: 10
  save_strategy: "epoch"
  evaluation_strategy: "epoch"    
  report_to: "wandb"
  group_by_length: true



# Saving Paths
saving:
  final_dir: "final_QA_COT_SFT_ckpt"       

  merged_model_dir: "../cot/cot_model/halueval/gemma"  
  # final merged full model
